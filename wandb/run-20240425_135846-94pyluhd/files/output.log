Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/home/iarroyof/miniconda3/envs/pt/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Epoch: 0 | Train Loss: 9.245375633239746
Epoch: 0 | Train Loss: 3.322538375854492
Epoch: 0 | Train Loss: 1.774092435836792
Epoch: 0 | Train Loss: 2.7521791458129883
Epoch: 0 | Train Loss: 2.682896375656128
Epoch: 0 | Train Loss: 2.330864667892456
Epoch: 0 | Train Loss: 1.594696044921875
Epoch: 0 | Train Loss: 1.932621955871582
Epoch: 0 | Train Loss: 2.2726149559020996
Epoch: 0 | Train Loss: 2.234234094619751
Epoch: 0 | Train Loss: 1.7787996530532837
Epoch: 0 | Train Loss: 1.784788727760315
Epoch: 0 | Train Loss: 2.8820135593414307
Epoch: 0 | Train Loss: 2.0643160343170166
Epoch: 0 | Train Loss: 1.9098694324493408
Epoch: 0 | Train Loss: 1.7302130460739136
Epoch: 0 | Train Loss: 2.3754754066467285
Epoch: 0 | Train Loss: 2.0862374305725098
Epoch: 0 | Train Loss: 2.537964105606079
Epoch: 0 | Train Loss: 1.539833664894104
Epoch: 0 | Train Loss: 2.104454755783081
Epoch: 0 | Train Loss: 2.048776626586914
Epoch: 0 | Train Loss: 1.5926616191864014
Epoch: 0 | Train Loss: 2.107466459274292
Epoch: 0 | Train Loss: 2.0157322883605957
Epoch: 0 | Train Loss: 2.2926743030548096
Epoch: 0 | Train Loss: 2.5198471546173096
Epoch: 0 | Train Loss: 2.7341864109039307
Epoch: 0 | Train Loss: 2.218419075012207
Epoch: 0 | Train Loss: 2.57338547706604
Epoch: 0 | Train Loss: 2.5056583881378174
Epoch: 0 | Train Loss: 2.3899612426757812
Epoch: 0 | Train Loss: 2.0981173515319824
Epoch: 0 | Train Loss: 1.9682533740997314
Epoch: 0 | Train Loss: 1.9750417470932007
Epoch: 0 | Train Loss: 2.5711171627044678
Epoch: 0 | Train Loss: 1.8132681846618652
Epoch: 0 | Train Loss: 2.4517226219177246
Epoch: 0 | Train Loss: 2.4609832763671875
Epoch: 0 | Train Loss: 1.7862532138824463
Epoch: 0 | Train Loss: 1.8705546855926514
Epoch: 0 | Train Loss: 2.016425848007202
Epoch: 0 | Train Loss: 2.4080071449279785
Epoch: 0 | Train Loss: 1.8859690427780151
Epoch: 0 | Train Loss: 2.6455228328704834
Epoch: 0 | Train Loss: 1.9502853155136108
Epoch: 0 | Train Loss: 1.9635632038116455
Epoch: 0 | Train Loss: 1.6514699459075928
Epoch: 0 | Train Loss: 2.80487322807312
Epoch: 0 | Train Loss: 1.8829329013824463
Epoch: 0 | Train Loss: 1.873693585395813
Epoch: 0 | Train Loss: 2.576904773712158
Epoch: 0 | Train Loss: 1.9424293041229248
Epoch: 0 | Train Loss: 2.576120615005493
Epoch: 0 | Train Loss: 2.3125877380371094
Epoch: 0 | Train Loss: 2.464808702468872
Epoch: 0 | Train Loss: 2.1886186599731445
Epoch: 0 | Train Loss: 1.9046508073806763
Epoch: 0 | Train Loss: 1.570063829421997
Epoch: 0 | Train Loss: 1.7878906726837158
Epoch: 0 | Train Loss: 1.9019135236740112
Epoch: 0 | Train Loss: 1.9404584169387817
Epoch: 0 | Train Loss: 1.7515400648117065
Epoch: 0 | Train Loss: 2.593641757965088
Epoch: 0 | Train Loss: 2.5065386295318604
Epoch: 0 | Train Loss: 2.7494778633117676
Epoch: 0 | Train Loss: 2.253159761428833
Epoch: 0 | Train Loss: 2.1685163974761963
Epoch: 0 | Train Loss: 2.040151834487915
Epoch: 0 | Train Loss: 1.4416216611862183
Epoch: 0 | Train Loss: 2.115682601928711
Epoch: 0 | Train Loss: 1.9974099397659302
Epoch: 0 | Train Loss: 2.543931245803833
Epoch: 0 | Train Loss: 2.4925336837768555
Epoch: 0 | Train Loss: 2.6938247680664062
Epoch: 0 | Train Loss: 1.8245933055877686
Epoch: 0 | Train Loss: 2.038046360015869
Epoch: 0 | Train Loss: 2.2190492153167725
Epoch: 0 | Train Loss: 2.410611629486084
Epoch: 0 | Train Loss: 2.0041348934173584
Epoch: 0 | Train Loss: 2.2699296474456787
Epoch: 0 | Train Loss: 2.1789567470550537
Epoch: 0 | Train Loss: 2.043992042541504
Epoch: 0 | Train Loss: 1.92369544506073
Epoch: 0 | Train Loss: 2.7105188369750977
Epoch: 0 | Train Loss: 2.0605244636535645
Epoch: 0 | Train Loss: 2.949399471282959
Epoch: 0 | Train Loss: 2.222466468811035
Epoch: 0 | Train Loss: 1.375624418258667
Epoch: 0 | Train Loss: 1.924459457397461
Epoch: 0 | Train Loss: 1.283907175064087
Epoch: 0 | Train Loss: 1.9480639696121216
Epoch: 0 | Train Loss: 2.1528818607330322
Epoch: 0 | Train Loss: 2.4314379692077637
Epoch: 0 | Train Loss: 2.3568882942199707
Epoch: 0 | Train Loss: 2.3531038761138916
Epoch: 0 | Train Loss: 1.8309584856033325
Epoch: 0 | Train Loss: 2.1362404823303223
Epoch: 0 | Train Loss: 1.9324448108673096
Epoch: 0 | Train Loss: 1.8798354864120483
Epoch: 0 | Train Loss: 2.4444124698638916
Epoch: 0 | Train Loss: 2.3519256114959717
Epoch: 0 | Train Loss: 2.8145930767059326
Epoch: 0 | Train Loss: 2.7291135787963867
Epoch: 0 | Train Loss: 2.092494487762451
Epoch: 0 | Train Loss: 2.4937801361083984
Epoch: 0 | Train Loss: 2.347058057785034
Epoch: 0 | Train Loss: 1.9970548152923584
Epoch: 0 | Train Loss: 2.3838629722595215
Epoch: 0 | Train Loss: 2.3787052631378174
Epoch: 0 | Train Loss: 2.5048093795776367
Epoch: 0 | Train Loss: 2.082822322845459
Epoch: 0 | Train Loss: 2.0407469272613525
Epoch: 0 | Train Loss: 2.5982143878936768
Epoch: 0 | Train Loss: 1.4922999143600464
Epoch: 0 | Train Loss: 2.5802199840545654
Epoch: 0 | Train Loss: 2.1074378490448
Epoch: 0 | Train Loss: 2.4978344440460205
Epoch: 0 | Train Loss: 1.967794418334961
Epoch: 0 | Train Loss: 2.1364383697509766
Epoch: 0 | Train Loss: 1.5565285682678223
Epoch: 0 | Train Loss: 2.4859533309936523
Epoch: 0 | Train Loss: 2.087207078933716
Epoch: 0 | Train Loss: 2.810407876968384
Epoch: 0 | Train Loss: 2.344296932220459
Epoch: 0 | Train Loss: 2.015582323074341
Epoch: 0 | Train Loss: 1.7738027572631836
Epoch: 0 | Train Loss: 2.0512213706970215
Epoch: 0 | Train Loss: 1.7260509729385376
Epoch: 0 | Train Loss: 2.124126672744751
Epoch: 0 | Train Loss: 1.6328309774398804
Epoch: 0 | Train Loss: 2.0694282054901123
Epoch: 0 | Train Loss: 1.720013976097107
Epoch: 0 | Train Loss: 2.177809238433838
Epoch: 0 | Train Loss: 2.3290109634399414
Epoch: 0 | Train Loss: 2.2041337490081787
Epoch: 0 | Train Loss: 2.276845693588257
Epoch: 0 | Train Loss: 1.9312313795089722
Epoch: 0 | Train Loss: 2.30098557472229
Epoch: 0 | Train Loss: 1.6164528131484985
Epoch: 0 | Train Loss: 1.768035888671875
Epoch: 0 | Train Loss: 2.325273036956787
Epoch: 0 | Train Loss: 2.3144171237945557
Epoch: 0 | Train Loss: 2.3445394039154053
Epoch: 0 | Train Loss: 2.2980458736419678
Epoch: 0 | Train Loss: 1.9900004863739014
Epoch: 0 | Train Loss: 1.9472061395645142
Epoch: 0 | Train Loss: 2.828058958053589
Epoch: 0 | Train Loss: 1.4983892440795898
Epoch: 0 | Train Loss: 2.26098895072937
Epoch: 0 | Train Loss: 2.4532651901245117
Epoch: 0 | Train Loss: 2.623894691467285
Epoch: 0 | Train Loss: 2.274057388305664
Epoch: 0 | Train Loss: 1.6569091081619263
Epoch: 0 | Train Loss: 2.4833810329437256
Epoch: 0 | Train Loss: 1.6997548341751099
Epoch: 0 | Train Loss: 2.4005048274993896
Epoch: 0 | Train Loss: 1.4651527404785156
Epoch: 0 | Train Loss: 2.032020330429077
Epoch: 0 | Train Loss: 2.7054684162139893
Epoch: 0 | Train Loss: 1.9051849842071533
Epoch: 0 | Train Loss: 2.032027006149292
Epoch: 0 | Train Loss: 2.030919075012207
Epoch: 0 | Train Loss: 2.229933023452759
Epoch: 0 | Train Loss: 2.0447933673858643
Epoch: 0 | Train Loss: 2.161815881729126
Epoch: 0 | Train Loss: 2.016608238220215
Epoch: 0 | Train Loss: 2.391547679901123
Epoch: 0 | Train Loss: 1.8288787603378296
Epoch: 0 | Train Loss: 1.4889442920684814
Epoch: 0 | Train Loss: 1.6682778596878052
Epoch: 0 | Train Loss: 2.2530198097229004
Epoch: 0 | Train Loss: 1.869215726852417
Epoch: 0 | Train Loss: 2.1234333515167236
Epoch: 0 | Train Loss: 1.6560038328170776
Epoch: 0 | Train Loss: 2.06683611869812
Epoch: 0 | Train Loss: 1.9358575344085693
Epoch: 0 | Train Loss: 2.811957836151123
Epoch: 0 | Train Loss: 2.112004518508911
Epoch: 0 | Train Loss: 2.72072434425354
Epoch: 0 | Train Loss: 2.3996520042419434
Epoch: 0 | Train Loss: 2.626178741455078
Epoch: 0 | Train Loss: 2.241525173187256
Epoch: 0 | Train Loss: 1.9065601825714111
Epoch: 0 | Train Loss: 2.103159189224243
Epoch: 0 | Train Loss: 1.9064128398895264
Epoch: 0 | Train Loss: 2.2877981662750244
Epoch: 0 | Train Loss: 1.9129565954208374
Epoch: 0 | Train Loss: 1.6742608547210693
Epoch: 0 | Train Loss: 2.316654682159424
Epoch: 0 | Train Loss: 2.5146443843841553
Epoch: 0 | Train Loss: 2.0888404846191406
Epoch: 0 | Train Loss: 2.300849437713623
Epoch: 0 | Train Loss: 1.8309191465377808
Epoch: 0 | Train Loss: 1.8474293947219849
Epoch: 0 | Train Loss: 1.8939568996429443
Epoch: 0 | Train Loss: 2.379793882369995
Epoch: 0 | Train Loss: 1.7313008308410645
Epoch: 0 | Train Loss: 2.042759895324707
Epoch: 0 | Train Loss: 2.1836204528808594
Epoch: 0 | Test Loss: 2.432074785232544
Epoch: 0 | Test Loss: 1.550918698310852
Epoch: 0 | Test Loss: 1.4040544033050537
Epoch: 0 | Test Loss: 2.123472213745117
Epoch: 0 | Test Loss: 2.5701260566711426
Epoch: 0 | Test Loss: 2.135051727294922
Epoch: 0 | Test Loss: 3.0354843139648438
Epoch: 0 | Test Loss: 1.531117558479309
Epoch: 0 | Test Loss: 1.734217882156372
Epoch: 0 | Test Loss: 2.1093459129333496
Epoch: 0 | Test Loss: 1.9480448961257935
Epoch: 0 | Test Loss: 3.0427229404449463
Epoch: 0 | Test Loss: 2.5149080753326416
Epoch: 0 | Test Loss: 2.8879611492156982
Epoch: 0 | Test Loss: 2.597764730453491
Epoch: 0 | Test Loss: 2.503746271133423
Epoch: 0 | Test Loss: 2.8353021144866943
Epoch: 0 | Test Loss: 2.685715436935425
Epoch: 0 | Test Loss: 1.9261540174484253
Epoch: 0 | Test Loss: 1.3417192697525024
Epoch: 0 | Test Loss: 2.3737592697143555
Epoch: 0 | Test Loss: 1.7719504833221436
Epoch: 0 | Test Loss: 2.3699188232421875
Epoch: 0 | Test Loss: 2.6758182048797607
Epoch: 0 | Test Loss: 1.9057700634002686
Epoch: 0 | Test Loss: 1.4190011024475098
Epoch: 0 | Test Loss: 1.8578193187713623
Epoch: 0 | Test Loss: 2.4121525287628174
Epoch: 0 | Test Loss: 2.449681282043457
Epoch: 0 | Test Loss: 2.5239274501800537
Epoch: 0 | Test Loss: 1.8119524717330933
Epoch: 0 | Test Loss: 1.3902971744537354
Epoch: 0 | Test Loss: 1.926720380783081
Epoch: 0 | Test Loss: 2.2560930252075195
Epoch: 0 | Test Loss: 1.3534396886825562
Epoch: 0 | Test Loss: 2.8400466442108154
Epoch: 0 | Test Loss: 2.4546773433685303
Epoch: 0 | Test Loss: 2.3084874153137207
Epoch: 0 | Test Loss: 1.3509142398834229
Epoch: 0 | Test Loss: 2.6386077404022217
Epoch: 0 | Test Loss: 2.7520217895507812
Epoch: 0 | Test Loss: 2.5096607208251953
Epoch: 0 | Test Loss: 2.0508642196655273
Epoch: 0 | Test Loss: 1.8928745985031128
Epoch: 0 | Test Loss: 1.5878350734710693
Epoch: 0 | Test Loss: 2.0799782276153564
Epoch: 0 | Test Loss: 0.7492460012435913
Epoch: 0 | Test Loss: 2.3447389602661133
Epoch: 0 | Test Loss: 3.0355453491210938
Epoch: 0 | Test Loss: 1.9628815650939941
Epoch: 0 | Test Loss: 1.355232834815979
Epoch: 0 | Test Loss: 2.12813401222229
Epoch: 0 | Test Loss: 2.919400930404663
Epoch: 0 | Test Loss: 2.864259958267212
Epoch: 0 | Test Loss: 2.037666082382202
Epoch: 0 | Test Loss: 1.4232903718948364
Epoch: 0 | Test Loss: 2.4867334365844727
Epoch: 0 | Test Loss: 1.3464950323104858
Epoch: 0 | Test Loss: 2.518723726272583
Epoch: 0 | Test Loss: 1.807059407234192
Epoch: 0 | Test Loss: 2.8345329761505127
Epoch: 0 | Test Loss: 1.7581959962844849
Epoch: 0 | Test Loss: 1.1081328392028809
Epoch: 0 | Test Loss: 1.3873952627182007
Epoch: 0 | Test Loss: 1.8922245502471924
Epoch: 0 | Test Loss: 2.1889219284057617
Epoch: 0 | Test Loss: 1.978222370147705
Epoch: 0 | Test Loss: 2.397139310836792
Epoch: 0 | Test Loss: 1.5987279415130615
Epoch: 0 | Test Loss: 1.5857689380645752
Epoch: 0 | Test Loss: 2.5149364471435547
Epoch: 0 | Test Loss: 2.9563791751861572
Epoch: 0 | Test Loss: 2.1431398391723633
Epoch: 0 | Test Loss: 3.15095853805542
Epoch: 0 | Test Loss: 2.1440229415893555
Epoch: 0 | Test Loss: 2.273667573928833
Epoch: 0 | Test Loss: 3.281240701675415
Epoch: 0 | Test Loss: 1.5787264108657837
Epoch: 0 | Test Loss: 2.983503818511963
Epoch: 0 | Test Loss: 1.308472752571106
Epoch: 0 | Test Loss: 2.3095953464508057
Epoch: 0 | Test Loss: 2.1140153408050537
Epoch: 0 | Test Loss: 2.473620891571045
Epoch: 0 | Test Loss: 2.3948442935943604
Epoch: 0 | Test Loss: 2.0328047275543213
Epoch: 0 | Test Loss: 2.37703800201416
Epoch: 0 | Test Loss: 2.4241783618927
Epoch: 0 | Test Loss: 1.6591075658798218
Epoch: 0 | Test Loss: 2.155076265335083
Epoch: 0 | Test Loss: 1.5206774473190308
Epoch: 0 | Test Loss: 2.2337281703948975
Epoch: 0 | Test Loss: 2.781938076019287
Epoch: 0 | Test Loss: 1.421301245689392
Epoch: 0 | Test Loss: 2.796187400817871
Epoch: 0 | Test Loss: 0.9372349977493286
Epoch: 0 | Test Loss: 1.891721487045288
Epoch: 0 | Test Loss: 1.7657822370529175
Epoch: 0 | Test Loss: 2.1783502101898193
Epoch: 0 | Test Loss: 1.7665342092514038
Epoch: 0 | Test Loss: 2.307135581970215
Epoch: 0 | Test Loss: 2.0507266521453857
Epoch: 0 | Test Loss: 2.348191261291504
Epoch: 0 | Test Loss: 1.3582698106765747
Epoch: 0 | Test Loss: 2.310955286026001
Epoch: 0 | Test Loss: 1.566253662109375
Epoch: 0 | Test Loss: 2.044088125228882
Epoch: 0 | Test Loss: 1.7503021955490112
Epoch: 0 | Test Loss: 1.429720401763916
Epoch: 0 | Test Loss: 1.8214565515518188
Epoch: 0 | Test Loss: 1.7862417697906494
Epoch: 0 | Test Loss: 2.178544044494629
Epoch: 0 | Test Loss: 1.7215322256088257
Epoch: 0 | Test Loss: 1.651962399482727
Epoch: 0 | Test Loss: 1.8230551481246948
Epoch: 0 | Test Loss: 1.8233730792999268
Epoch: 0 | Test Loss: 1.9470347166061401
Epoch: 0 | Test Loss: 1.3903709650039673
Epoch: 0 | Test Loss: 2.1822760105133057
Epoch: 0 | Test Loss: 1.6828382015228271
Epoch: 0 | Test Loss: 1.477512001991272
Epoch: 0 | Test Loss: 2.997037410736084
Epoch: 0 | Test Loss: 0.9132434129714966
Epoch: 0 | Test Loss: 2.3896541595458984
Epoch: 0 | Test Loss: 2.0578436851501465
Epoch: 0 | Test Loss: 2.3599720001220703
Epoch: 0 | Test Loss: 1.7377064228057861
Epoch: 0 | Test Loss: 1.8777940273284912
Epoch: 0 | Test Loss: 1.2613548040390015
Epoch: 0 | Test Loss: 2.157597541809082
Epoch: 0 | Test Loss: 2.2976014614105225
Epoch: 0 | Test Loss: 1.953829050064087
Epoch: 0 | Test Loss: 1.7472559213638306
Epoch: 0 | Test Loss: 2.5759377479553223
Epoch: 0 | Test Loss: 1.9093382358551025
Epoch: 0 | Test Loss: 1.1144068241119385
Epoch: 0 | Test Loss: 2.4162185192108154
Epoch: 0 | Test Loss: 1.7515751123428345
Epoch: 0 | Test Loss: 0.9554474353790283
Epoch: 0 | Test Loss: 1.825909972190857
Epoch: 0 | Test Loss: 1.2698348760604858
Epoch: 0 | Test Loss: 1.8673992156982422
Epoch: 0 | Test Loss: 2.0014359951019287
Epoch: 0 | Test Loss: 2.4658470153808594
Epoch: 0 | Test Loss: 1.9611321687698364
Epoch: 0 | Test Loss: 2.4789650440216064
Epoch: 0 | Test Loss: 0.8859224915504456
Epoch: 0 | Test Loss: 2.0209169387817383
Epoch: 0 | Test Loss: 1.9378868341445923
Epoch: 0 | Test Loss: 2.2633602619171143
Epoch: 0 | Test Loss: 1.9329251050949097
Epoch: 0 | Test Loss: 2.3168249130249023
Epoch: 0 | Test Loss: 1.8763220310211182
Epoch: 0 | Test Loss: 1.9792009592056274
Epoch: 0 | Test Loss: 1.990064024925232
Epoch: 0 | Test Loss: 1.3229453563690186
Epoch: 0 | Test Loss: 1.6875386238098145
Epoch: 0 | Test Loss: 2.511753797531128
Epoch: 0 | Test Loss: 1.8169382810592651
Epoch: 0 | Test Loss: 2.58508038520813
Epoch: 0 | Test Loss: 2.3854050636291504
Epoch: 0 | Test Loss: 2.0619490146636963
Epoch: 0 | Test Loss: 2.5276331901550293
Epoch: 0 | Test Loss: 2.3066799640655518
Epoch: 0 | Test Loss: 1.1604881286621094
Epoch: 0 | Test Loss: 1.2247692346572876
Epoch: 0 | Test Loss: 2.353689193725586
Epoch: 0 | Test Loss: 2.7911417484283447
Epoch: 0 | Test Loss: 2.4127655029296875
Epoch: 0 | Test Loss: 1.705363392829895
Epoch: 0 | Test Loss: 2.077303171157837
Epoch: 0 | Test Loss: 2.6250159740448
Epoch: 0 | Test Loss: 1.8307071924209595
Epoch: 0 | Test Loss: 2.803478717803955
Epoch: 0 | Test Loss: 2.1052727699279785
Epoch: 0 | Test Loss: 1.2420216798782349
Epoch: 0 | Test Loss: 2.0912113189697266
Epoch: 0 | Test Loss: 2.188671350479126
Epoch: 0 | Test Loss: 1.479026436805725
Epoch: 0 | Test Loss: 2.506007432937622
Epoch: 0 | Test Loss: 2.4593453407287598
Epoch: 0 | Test Loss: 2.4335362911224365
Epoch: 0 | Test Loss: 0.879062294960022
Epoch: 0 | Test Loss: 3.222438335418701
Epoch: 0 | Test Loss: 2.093632698059082
Epoch: 0 | Test Loss: 2.6521379947662354
Epoch: 0 | Test Loss: 1.523363471031189
Epoch: 0 | Test Loss: 1.8098124265670776
Epoch: 0 | Test Loss: 1.3290817737579346
Epoch: 0 | Test Loss: 1.2719372510910034
Epoch: 0 | Test Loss: 2.188072681427002
Epoch: 0 | Test Loss: 1.07650625705719
Epoch: 0 | Test Loss: 1.9667280912399292
Epoch: 0 | Test Loss: 1.845313310623169
Epoch: 0 | Test Loss: 3.269073247909546
Epoch: 0 | Test Loss: 2.110136032104492
Epoch: 0 | Test Loss: 1.0962103605270386
Epoch: 0 | Test Loss: 2.3551347255706787
Epoch: 0 | Test Loss: 1.5479259490966797
Epoch: 0 | Test Loss: 1.6717891693115234
Epoch: 0 | Test Loss: 1.8857489824295044
Traceback (most recent call last):
  File "/home/iarroyof/Projects/ov-llm-reasoning/seq2seq_T5.py", line 207, in <module>
  File "/home/iarroyof/Projects/ov-llm-reasoning/seq2seq_T5.py", line 205, in main
    reasoning_pipeline.test(val_loader, epoch)
  File "/home/iarroyof/Projects/ov-llm-reasoning/seq2seq_T5.py", line 84, in generate
    gen_score = self.calculate_validation_score(data, generated_ids)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/iarroyof/Projects/ov-llm-reasoning/seq2seq_T5.py", line 124, in calculate_validation_score
    bleu_score = bleu.corpus_score([[ref] for ref in target_text], generated_text).score
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/iarroyof/miniconda3/envs/pt/lib/python3.11/site-packages/sacrebleu/metrics/base.py", line 414, in corpus_score
    self._check_corpus_score_args(hypotheses, references)
  File "/home/iarroyof/miniconda3/envs/pt/lib/python3.11/site-packages/sacrebleu/metrics/base.py", line 258, in _check_corpus_score_args
    raise TypeError(f'{prefix}: {err_msg}')
TypeError: BLEU: Each element of `hyps` should be a string.