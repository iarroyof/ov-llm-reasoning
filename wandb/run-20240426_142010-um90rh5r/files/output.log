Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/home/iarroyof/miniconda3/envs/pt/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Epoch: 0 | Train Loss: 9.439518928527832
Epoch: 0 | Train Loss: 7.567396640777588
Epoch: 0 | Train Loss: 7.220188617706299
Epoch: 0 | Train Loss: 7.184043884277344
Epoch: 0 | Train Loss: 7.243096828460693
Epoch: 0 | Train Loss: 7.220119953155518
Epoch: 0 | Train Loss: 7.337094783782959
Epoch: 0 | Train Loss: 7.179882526397705
Epoch: 0 | Train Loss: 7.0731072425842285
Epoch: 0 | Train Loss: 7.069363117218018
Epoch: 0 | Train Loss: 7.276418685913086
Epoch: 0 | Train Loss: 7.059710502624512
Epoch: 0 | Train Loss: 7.220892429351807
Epoch: 0 | Train Loss: 7.1043171882629395
Epoch: 0 | Train Loss: 7.140904903411865
Epoch: 0 | Train Loss: 6.976881980895996
Epoch: 0 | Train Loss: 7.102723598480225
Epoch: 0 | Train Loss: 7.052231311798096
Epoch: 0 | Train Loss: 7.063628673553467
Epoch: 0 | Train Loss: 7.1102986335754395
Epoch: 0 | Train Loss: 7.11640739440918
Epoch: 0 | Train Loss: 7.073825836181641
Epoch: 0 | Train Loss: 6.923767566680908
Epoch: 0 | Train Loss: 7.117921352386475
Epoch: 0 | Train Loss: 7.075857639312744
Epoch: 0 | Train Loss: 6.970930099487305
Epoch: 0 | Train Loss: 7.089175701141357
Epoch: 0 | Train Loss: 7.018813133239746