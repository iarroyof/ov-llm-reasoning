Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/home/iarroyof/miniconda3/envs/pt/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Epoch: 0 | Train Loss: 7.683215618133545
Epoch: 0 | Train Loss: 2.644237995147705
Epoch: 0 | Train Loss: 2.895235061645508
Epoch: 0 | Train Loss: 2.178276777267456
Epoch: 0 | Train Loss: 2.4434244632720947
Epoch: 0 | Train Loss: 2.173933744430542
Epoch: 0 | Train Loss: 1.8709174394607544
Epoch: 0 | Train Loss: 2.0471372604370117
Epoch: 0 | Train Loss: 2.0177454948425293
Epoch: 0 | Train Loss: 2.31673526763916
Epoch: 0 | Train Loss: 2.4248101711273193
Epoch: 0 | Train Loss: 2.9030025005340576
Epoch: 0 | Train Loss: 1.9811761379241943
Epoch: 0 | Train Loss: 2.084699869155884
Epoch: 0 | Train Loss: 1.9382517337799072
Epoch: 0 | Train Loss: 2.094924211502075
Epoch: 0 | Train Loss: 1.980202555656433
Epoch: 0 | Train Loss: 2.547903299331665
Epoch: 0 | Train Loss: 2.6346700191497803
Epoch: 0 | Train Loss: 2.8051376342773438
Epoch: 0 | Train Loss: 2.5850441455841064
Epoch: 0 | Train Loss: 1.8721590042114258
Epoch: 0 | Train Loss: 2.4464640617370605
Epoch: 0 | Train Loss: 2.126788854598999
Epoch: 0 | Train Loss: 1.8275479078292847
Epoch: 0 | Train Loss: 1.9657366275787354
Epoch: 0 | Train Loss: 2.3280436992645264
Epoch: 0 | Train Loss: 2.2604196071624756
Epoch: 0 | Train Loss: 2.386650800704956
Epoch: 0 | Train Loss: 1.560334324836731
Epoch: 0 | Train Loss: 2.2000668048858643
Epoch: 0 | Train Loss: 2.099778175354004
Epoch: 0 | Train Loss: 2.966484785079956
Epoch: 0 | Train Loss: 2.1527295112609863
Epoch: 0 | Train Loss: 2.4875478744506836
Epoch: 0 | Train Loss: 2.129267692565918
Epoch: 0 | Train Loss: 2.312934398651123
Epoch: 0 | Train Loss: 1.8719995021820068
Epoch: 0 | Train Loss: 2.146118640899658
Epoch: 0 | Train Loss: 2.007141351699829
Epoch: 0 | Train Loss: 1.9503620862960815
Epoch: 0 | Train Loss: 2.452272891998291
Epoch: 0 | Train Loss: 2.0198862552642822
Epoch: 0 | Train Loss: 2.6719255447387695
Epoch: 0 | Train Loss: 2.3512279987335205
Epoch: 0 | Train Loss: 2.015528440475464
Epoch: 0 | Train Loss: 2.4304885864257812
Epoch: 0 | Train Loss: 1.7523308992385864
Epoch: 0 | Train Loss: 2.187201499938965
Epoch: 0 | Train Loss: 2.0253846645355225
Epoch: 0 | Train Loss: 1.8629568815231323
Epoch: 0 | Train Loss: 2.0804286003112793
Epoch: 0 | Train Loss: 2.6423208713531494
Epoch: 0 | Train Loss: 2.2794408798217773
Epoch: 0 | Train Loss: 2.3311023712158203
Epoch: 0 | Train Loss: 2.2146213054656982
Epoch: 0 | Train Loss: 2.112337827682495
Epoch: 0 | Train Loss: 1.8677533864974976
Epoch: 0 | Train Loss: 1.6014403104782104
Epoch: 0 | Train Loss: 2.742020606994629
Epoch: 0 | Train Loss: 1.3700413703918457
Epoch: 0 | Train Loss: 1.5935777425765991
Epoch: 0 | Train Loss: 3.0299808979034424
Epoch: 0 | Train Loss: 2.65763258934021
Epoch: 0 | Train Loss: 2.575181245803833
Epoch: 0 | Train Loss: 2.3532118797302246
Epoch: 0 | Train Loss: 2.392265558242798
Epoch: 0 | Train Loss: 1.8212207555770874
Epoch: 0 | Train Loss: 2.1417429447174072
Epoch: 0 | Train Loss: 1.9543540477752686
Epoch: 0 | Train Loss: 2.0967421531677246
Epoch: 0 | Train Loss: 2.0983879566192627
Epoch: 0 | Train Loss: 1.6984119415283203
Epoch: 0 | Train Loss: 2.639211416244507
Epoch: 0 | Train Loss: 1.9858417510986328
Epoch: 0 | Train Loss: 1.8306448459625244
Epoch: 0 | Train Loss: 2.0503833293914795
Epoch: 0 | Train Loss: 2.3443167209625244
Epoch: 0 | Train Loss: 2.7153286933898926
Epoch: 0 | Train Loss: 2.393984317779541
Epoch: 0 | Train Loss: 2.036550760269165
Epoch: 0 | Train Loss: 1.796342134475708
Epoch: 0 | Train Loss: 2.139249324798584
Epoch: 0 | Train Loss: 2.350449562072754
Epoch: 0 | Train Loss: 1.9096901416778564
Epoch: 0 | Train Loss: 1.9897658824920654
Epoch: 0 | Train Loss: 1.2915594577789307
Epoch: 0 | Train Loss: 1.8599319458007812
Epoch: 0 | Train Loss: 1.9740642309188843
Epoch: 0 | Train Loss: 2.148373603820801
Epoch: 0 | Train Loss: 1.8143677711486816
Epoch: 0 | Train Loss: 2.1242377758026123
Epoch: 0 | Train Loss: 2.2653982639312744
Epoch: 0 | Train Loss: 2.4565351009368896
Epoch: 0 | Train Loss: 2.1013476848602295
Epoch: 0 | Train Loss: 1.94150972366333
Epoch: 0 | Train Loss: 2.0435891151428223
Epoch: 0 | Train Loss: 2.0297601222991943
Epoch: 0 | Train Loss: 2.3685150146484375
Epoch: 0 | Train Loss: 1.5516636371612549
Epoch: 0 | Train Loss: 1.4038504362106323
Epoch: 0 | Train Loss: 2.6361939907073975
Epoch: 0 | Train Loss: 2.113373041152954
Epoch: 0 | Train Loss: 2.5286457538604736
Epoch: 0 | Train Loss: 2.038127899169922
Epoch: 0 | Train Loss: 2.132896900177002
Epoch: 0 | Train Loss: 2.208270788192749
Epoch: 0 | Train Loss: 1.7618968486785889
Epoch: 0 | Train Loss: 2.407695770263672
Epoch: 0 | Train Loss: 1.9858061075210571
Epoch: 0 | Train Loss: 2.3478333950042725
Epoch: 0 | Train Loss: 1.9102164506912231
Epoch: 0 | Train Loss: 1.755403995513916
Epoch: 0 | Train Loss: 2.1104822158813477
Epoch: 0 | Train Loss: 2.5866899490356445
Epoch: 0 | Train Loss: 2.4891088008880615
Epoch: 0 | Train Loss: 1.9775855541229248
Epoch: 0 | Train Loss: 2.5586562156677246
Epoch: 0 | Train Loss: 2.335690975189209
Epoch: 0 | Train Loss: 2.3928873538970947
Epoch: 0 | Train Loss: 1.7198723554611206
Epoch: 0 | Train Loss: 1.9755686521530151
Epoch: 0 | Train Loss: 2.050783634185791
Epoch: 0 | Train Loss: 1.9215624332427979
Epoch: 0 | Train Loss: 2.4809834957122803
Epoch: 0 | Train Loss: 1.7434883117675781
Epoch: 0 | Train Loss: 1.5105981826782227
Epoch: 0 | Train Loss: 2.2580995559692383
Epoch: 0 | Train Loss: 2.2876815795898438
Epoch: 0 | Train Loss: 1.848836064338684
Epoch: 0 | Train Loss: 2.3872182369232178
Epoch: 0 | Train Loss: 2.3693695068359375
Epoch: 0 | Train Loss: 2.536555767059326
Epoch: 0 | Train Loss: 1.9872575998306274
Epoch: 0 | Train Loss: 2.279848337173462
Epoch: 0 | Train Loss: 1.4332661628723145
Epoch: 0 | Train Loss: 1.883830189704895
Epoch: 0 | Train Loss: 2.528399705886841
Epoch: 0 | Train Loss: 2.400470018386841
Epoch: 0 | Train Loss: 2.204040288925171
Epoch: 0 | Train Loss: 2.283100128173828
Epoch: 0 | Train Loss: 2.140745162963867
Epoch: 0 | Train Loss: 1.9249517917633057
Epoch: 0 | Train Loss: 1.872710943222046
Epoch: 0 | Train Loss: 2.4722750186920166
Epoch: 0 | Train Loss: 2.325993299484253
Epoch: 0 | Train Loss: 2.1106762886047363
Epoch: 0 | Train Loss: 2.426636219024658
Epoch: 0 | Train Loss: 1.9508823156356812
Epoch: 0 | Train Loss: 2.05551815032959
Epoch: 0 | Train Loss: 2.0100345611572266
Epoch: 0 | Train Loss: 1.845047950744629
Epoch: 0 | Train Loss: 1.9768327474594116
Epoch: 0 | Train Loss: 2.5222818851470947
Epoch: 0 | Train Loss: 2.682258367538452
Epoch: 0 | Train Loss: 2.634061098098755
Epoch: 0 | Train Loss: 1.9932260513305664
Epoch: 0 | Train Loss: 2.241443634033203
Epoch: 0 | Train Loss: 1.597334623336792
Epoch: 0 | Train Loss: 2.388356924057007
Epoch: 0 | Train Loss: 2.0776314735412598
Epoch: 0 | Train Loss: 2.1781387329101562
Epoch: 0 | Train Loss: 1.7842304706573486
Epoch: 0 | Train Loss: 2.0566930770874023
Epoch: 0 | Train Loss: 2.2204627990722656
Epoch: 0 | Train Loss: 2.068777322769165
Epoch: 0 | Train Loss: 1.591819167137146
Epoch: 0 | Train Loss: 2.7900922298431396
Epoch: 0 | Train Loss: 2.0516088008880615
Epoch: 0 | Train Loss: 2.298125982284546
Epoch: 0 | Train Loss: 2.0951218605041504
Epoch: 0 | Train Loss: 2.33054256439209
Epoch: 0 | Train Loss: 2.352994203567505
Epoch: 0 | Train Loss: 2.467876434326172
Epoch: 0 | Train Loss: 2.336500883102417
Epoch: 0 | Train Loss: 2.4983794689178467
Epoch: 0 | Train Loss: 2.3733654022216797
Epoch: 0 | Train Loss: 2.558295965194702
Epoch: 0 | Train Loss: 1.3680757284164429
Epoch: 0 | Train Loss: 2.203789710998535
Epoch: 0 | Train Loss: 2.208052396774292
Epoch: 0 | Train Loss: 2.141796588897705
Epoch: 0 | Train Loss: 2.2527267932891846
Epoch: 0 | Train Loss: 1.9362766742706299
Epoch: 0 | Train Loss: 2.5099923610687256
Epoch: 0 | Train Loss: 1.618184208869934
Epoch: 0 | Train Loss: 2.1498823165893555
Epoch: 0 | Train Loss: 2.5256495475769043
Epoch: 0 | Train Loss: 1.530920386314392
Epoch: 0 | Train Loss: 2.073154926300049
Epoch: 0 | Train Loss: 1.986628770828247
Epoch: 0 | Train Loss: 2.617147922515869
Epoch: 0 | Train Loss: 2.151221513748169
Epoch: 0 | Train Loss: 1.9812651872634888
Epoch: 0 | Train Loss: 2.2160937786102295
Epoch: 0 | Train Loss: 1.7154861688613892
Epoch: 0 | Train Loss: 2.5344672203063965
Epoch: 0 | Train Loss: 1.6008161306381226
Epoch: 0 | Train Loss: 2.0626420974731445
Epoch: 0 | Train Loss: 2.0917916297912598
Traceback (most recent call last):
  File "/home/iarroyof/Projects/ov-llm-reasoning/seq2seq_T5.py", line 206, in <module>
    main()
  File "/home/iarroyof/Projects/ov-llm-reasoning/seq2seq_T5.py", line 203, in main
    reasoning_pipeline.test(val_loader, epoch)
  File "/home/iarroyof/Projects/ov-llm-reasoning/seq2seq_T5.py", line 50, in test
    source_ids, source_mask, y_ids, lm_labels = self.get_data(step, data)
                                                ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: OVNeuralReasoningPipeline.get_data() takes 2 positional arguments but 3 were given