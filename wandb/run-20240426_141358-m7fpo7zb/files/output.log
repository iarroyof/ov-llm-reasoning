Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
/home/iarroyof/miniconda3/envs/pt/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
Epoch: 0 | Train Loss: 9.394152641296387
Epoch: 0 | Train Loss: 7.3138885498046875
Epoch: 0 | Train Loss: 7.119269371032715
Epoch: 0 | Train Loss: 7.123525619506836
Epoch: 0 | Train Loss: 7.096621513366699
Epoch: 0 | Train Loss: 7.057797431945801
Epoch: 0 | Train Loss: 7.0691328048706055
Epoch: 0 | Train Loss: 7.117326736450195
Epoch: 0 | Train Loss: 7.101340293884277
Epoch: 0 | Train Loss: 7.0379791259765625
Epoch: 0 | Train Loss: 6.9921441078186035
Epoch: 0 | Train Loss: 7.037862300872803
Epoch: 0 | Train Loss: 7.075194358825684
Epoch: 0 | Train Loss: 6.919974327087402
Epoch: 0 | Train Loss: 7.054686546325684
Epoch: 0 | Train Loss: 6.991549491882324
Epoch: 0 | Train Loss: 6.880896091461182
Epoch: 0 | Train Loss: 6.82769250869751
Epoch: 0 | Train Loss: 6.887729644775391
Epoch: 0 | Train Loss: 6.84448766708374
Epoch: 0 | Train Loss: 6.810857772827148
Epoch: 0 | Train Loss: 6.85640811920166
Epoch: 0 | Train Loss: 6.724351406097412
Epoch: 0 | Train Loss: 6.735788822174072
Epoch: 0 | Train Loss: 6.726548671722412
Epoch: 0 | Test Loss: 6.805964469909668 | Test score (bleu): 0.0385525020104322
Epoch: 0 | Test Loss: 6.7385573387146 | Test score (bleu): 0.0340914145141803
Epoch: 0 | Test Loss: 6.692539215087891 | Test score (bleu): 0.03639710479270274
Epoch: 0 | Test Loss: 6.710122108459473 | Test score (bleu): 0.036739097088970316
Epoch: 0 | Test Loss: 6.878526210784912 | Test score (bleu): 0.024734000360814527
Epoch: 0 | Test Loss: 6.8150634765625 | Test score (bleu): 0.02721905206169078
Epoch: 0 | Test Loss: 6.9511284828186035 | Test score (bleu): 0.026996717286564626
Epoch: 0 | Test Loss: 6.8503875732421875 | Test score (bleu): 0.025717525504556017
Epoch: 0 | Test Loss: 6.752884387969971 | Test score (bleu): 0.027120949413776484
Epoch: 0 | Test Loss: 6.9722065925598145 | Test score (bleu): 0.02527653771081112
Epoch: 0 | Test Loss: 6.964054584503174 | Test score (bleu): 0.027084707833772093
Epoch: 0 | Test Loss: 6.899327278137207 | Test score (bleu): 0.024329057312800766
Epoch: 0 | Test Loss: 6.945001602172852 | Test score (bleu): 0.02608294287331354
Epoch: 1 | Train Loss: 6.771515369415283
Epoch: 1 | Train Loss: 6.644692420959473
Epoch: 1 | Train Loss: 6.629053115844727
Epoch: 1 | Train Loss: 6.7202653884887695
Epoch: 1 | Train Loss: 6.6945695877075195
Epoch: 1 | Train Loss: 6.681315898895264
Epoch: 1 | Train Loss: 6.535377025604248
Epoch: 1 | Train Loss: 6.621912002563477
Epoch: 1 | Train Loss: 6.564582824707031
Epoch: 1 | Train Loss: 6.622150897979736
Epoch: 1 | Train Loss: 6.634206295013428
Epoch: 1 | Train Loss: 6.586859226226807
Epoch: 1 | Train Loss: 6.791884422302246
Epoch: 1 | Train Loss: 6.621834754943848
Epoch: 1 | Train Loss: 6.662591457366943
Epoch: 1 | Train Loss: 6.634310722351074
Epoch: 1 | Train Loss: 6.542784214019775
Epoch: 1 | Train Loss: 6.599513530731201
Epoch: 1 | Train Loss: 6.589819431304932
Epoch: 1 | Train Loss: 6.836543560028076
Epoch: 1 | Train Loss: 6.670576572418213
Epoch: 1 | Train Loss: 6.676245212554932
Epoch: 1 | Train Loss: 6.756342887878418
Epoch: 1 | Train Loss: 6.612940788269043
Epoch: 1 | Train Loss: 6.655313491821289
Epoch: 1 | Test Loss: 6.756896018981934 | Test score (bleu): 0.0375142618962088
Epoch: 1 | Test Loss: 6.659088134765625 | Test score (bleu): 0.032849633152999144
Epoch: 1 | Test Loss: 6.632405757904053 | Test score (bleu): 0.035450916427126604
Epoch: 1 | Test Loss: 6.687863349914551 | Test score (bleu): 0.036406582374099515
Epoch: 1 | Test Loss: 6.811203479766846 | Test score (bleu): 0.02386477730573735
Epoch: 1 | Test Loss: 6.763409614562988 | Test score (bleu): 0.026295020507187548
Epoch: 1 | Test Loss: 6.891889572143555 | Test score (bleu): 0.02626968081999042
Epoch: 1 | Test Loss: 6.80122184753418 | Test score (bleu): 0.025717525504556017
Epoch: 1 | Test Loss: 6.688488483428955 | Test score (bleu): 0.02664074283144033
Epoch: 1 | Test Loss: 6.926957130432129 | Test score (bleu): 0.024828988489645806
Epoch: 1 | Test Loss: 6.901725769042969 | Test score (bleu): 0.025877888878738695
Epoch: 1 | Test Loss: 6.849616527557373 | Test score (bleu): 0.0241237304558431
Epoch: 1 | Test Loss: 6.9368743896484375 | Test score (bleu): 0.02586281397100849