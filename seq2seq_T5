import wandb
import transformers
from datasets import load_dataset
from torch.optim import Adam
import torch
from torch.utils.data import DataLoader,Dataset,RandomSampler,SequentialSampler
from transformers import T5ForConditionalGeneration,T5Tokenizer,T5PreTrainedModel # SentencePiece library is required to download pretrained t5tokenizer
# Let's try T5TokenizerFast
from transformers.models.t5 import T5TokenizerFast

class OVNeuralReasoningPipeline:
    def __init__(self, model, tokenizer, device, gen_test_score='bleu'):
        self.model = model
        self.tokenizer = tokenizer
        self.device = device
        self.score_type = gen_test_score

    def get_data(self, data):
        source_ids, source_mask, target_ids = data["source_ids"].to(self.device), data["source_masks"].to(self.device), data["target_ids"].to(self.device)
        y_ids = target_ids[:, :-1].contiguous()
        lm_labels = target_ids[:, 1:].clone().detach()
        lm_labels[lm_labels == self.tokenizer.pad_token_id] = -100

        return source_ids, source_mask, y_ids, lm_labels
        
    def train(self, optimizer, loader, epoch):
        self.model.train()
        for step, data in enumerate(loader):
            source_ids, source_mask, y_ids, lm_labels = self.get_data(data)

            outputs = self.model(
                input_ids=source_ids,
                attention_mask=source_mask,
                decoder_input_ids=y_ids,
                labels=lm_labels,
            )
            loss = outputs[0]

            if step % 10 == 0:
                wandb.log({"training_loss": loss})
                print(f"Epoch: {epoch} | Train Loss: {loss}")

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    def test(self, loader, epoch):
        self.model.eval()
        for step, data in enumerate(loader):
            source_ids, source_mask, y_ids, lm_labels = self.get_data(step, data)

            outputs = self.model(
                input_ids=source_ids,
                attention_mask=source_mask,
                decoder_input_ids=y_ids,
                labels=lm_labels,
            )
            loss = outputs[0]
            
            if step % 10 == 0:
                wandb.log({"test_loss": loss})
                print(f"Epoch: {epoch} | Test Loss: {loss}")
                #wandb.log("test_score": )

    def generate(self, loader, return_predictions=False):
        self.model.eval()
        predictions, targets = [], []
        with torch.no_grad():
            for data in loader:
                source_ids, source_mask, target_ids = (data["source_ids"].to(self.device),
                                                       data["source_masks"].to(self.device),
                                                       data["target_ids"].to(self.device))
                generated_ids = self.model.generate(
                    input_ids=source_ids,
                    attention_mask=source_mask,
                    num_beams=2,
                    max_length=170,
                    repetition_penalty=2.5,
                    early_stopping=True,
                    length_penalty=1.0,
                )
                gen_score = self.calculate_validation_score(data, generated_ids)
                wandb.log({"generation_score": gen_score})

                if return_predictions:
                    preds = [self.tokenizer.decode(
                        p, skip_special_tokens=True, clean_up_tokenization_spaces=False)
                            for p in generated_ids]
                    target = [self.tokenizer.decode(
                        t, skip_special_tokens=True, clean_up_tokenization_spaces=False)
                            for t in target_ids]

                    predictions.extend(preds)
                    targets.extend(target)

                    return predictions, targets

    def calculate_validation_score(self, data, generated_ids):
        """
        Calculates validation loss using BLEU and ROUGE scores.

        Args:
            data: A dictionary containing source_ids, source_masks, and target_ids.
            generated_ids: A list of generated summaries (tensor after decoding).
            score_type: select either 'bleu', 'rouge', 'combined', 'all' (this 
                latter returns a list with all scores)

        Returns:
            A float representing the average validation loss (combination of BLEU and ROUGE).
        """
        target_ids = data["target_ids"].to(self.device)

        # Decode target summaries
        target_text = [self.tokenizer.decode(t, skip_special_tokens=True) for t in target_ids]

        # Decode generated summaries
        generated_text = [self.tokenizer.decode(p, skip_special_tokens=True) for p in generated_ids]

        # Calculate BLEU score
        if self.score_type in ['all', 'bleu', 'combined']:
            from sacrebleu import BLEU  # Install sacrebleu library: pip install sacrebleu
            bleu = BLEU(smooth_method='floor')
            bleu_score = bleu.corpus_score([[ref] for ref in target_text], generated_text)["bleu"]

        # Calculate ROUGE score (using py-rouge)
        if self.score_type in ['all', 'rouge', 'combined']:
            from rouge import Rouge  # Install py-rouge library: pip install rouge
            rouge = Rouge()
            rouge_score = rouge.get_scores(target_text, generated_text, avg=True)["rouge-l"]["f"]

        if self.score_type == 'combined':
        # Combine BLEU and ROUGE scores (weighted average is common)
            score = (bleu_score + rouge_score) / 2.0  # Adjust weights as needed
        if self.score_type == 'all':
            score = [bleu_score, rouge_score, score]
        elif self.score_type == 'bleu':
            score = bleu_score
        elif self.score_type == 'rouge':
            score = rouge_score
        return score

class CustomDataset(Dataset):
  def __init__(self,dataset,tokenizer,source_len,summ_len):
    self.dataset = dataset 
    self.tokenizer = tokenizer
    self.text_len = source_len
    self.summ_len = summ_len
    self.text = self.dataset['article']
    self.summary = self.dataset['highlights']

  def __len__(self):
    return len(self.text)

  def __getitem__(self,i):
    summary = str(self.summary[i])
    summary = ' '.join(summary.split())
    text = str(self.text[i])
    text = ' '.join(text.split())
    source = self.tokenizer.batch_encode_plus([text],max_length=self.text_len,return_tensors='pt',pad_to_max_length=True) # Each source sequence is encoded and padded to max length in batches
    target = self.tokenizer.batch_encode_plus([summary],max_length=self.summ_len,return_tensors='pt',pad_to_max_length=True) # Each target sequence is encoded and padded to max lenght in batches

    source_ids = source['input_ids'].squeeze()
    source_masks = source['attention_mask'].squeeze()
    target_ids = target['input_ids'].squeeze()
    target_masks = target['attention_mask'].squeeze()

    return {
        'source_ids':source_ids.to(torch.long),
        'source_masks':source_masks.to(torch.long),
        'target_ids':target_ids.to(torch.long),
        'target_masks':target_masks.to(torch.long)
    }
  

def main():
  wandb.init(project='huggingface')
  epochs = 2
  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  model_name = 't5-base'
  tokenizer = T5TokenizerFast.from_pretrained(model_name)
  model = T5ForConditionalGeneration.from_pretrained().to(device)  
  ## Prepare Dataset ##
  ##  We will use cnn_dailymail summarization dataset for abstractive summarization #
  dataset = load_dataset('cnn_dailymail','3.0.0')
  # As we can observe, dataset is too large so for now we will consider just 8k rows for training
  #  and 4k rows for validation
  train_dataset = dataset['train'][:8000]
  val_dataset = dataset['validation'][:4000]

  train_dataset = CustomDataset(train_dataset,tokenizer,270,160)
  val_dataset = CustomDataset(val_dataset,tokenizer,270,160)
  
  train_loader = DataLoader(dataset=train_dataset,batch_size=4,shuffle=True,num_workers=0)
  val_loader = DataLoader(dataset = val_dataset,batch_size=2,num_workers=0)
  
  reasoning_pipeline = OVNeuralReasoningPipeline(model, tokenizer, device)
 
  optimizer = Adam(model.parameters(), lr=3e-4, amsgrad=True)
  wandb.watch(model, log='all')
  # Call train function
  for epoch in range(epochs): #optimizer, train_loader, epoch
    reasoning_pipeline.train(optimizer, train_loader, epoch)
    reasoning_pipeline.test(val_loader, epoch)
    reasoning_pipeline.generate(val_loader, epoch)

main()